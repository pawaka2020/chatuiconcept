Large scale table:

Improve read:
Indexes for rows

Improve write:
Batched table insertions
Queued table insertions

Improve both:
Redis caching layer
partitioning data across multiple database servers

--
-- accepts up to 1000 write requests from Flutter app.
-- Once after 1000 is reached, two arrays created instead.
-- example if 1500 requests are received, we will have 1000-size and 500-size arrays.
-- every 5 seconds this database will automatically write the array to the database.

CREATE OR REPLACE FUNCTION insert_users()
  RETURNS void AS
$BODY$
DECLARE
  users chat_users_large[];
BEGIN
  -- Get users to insert
  SELECT ARRAY_AGG(chat_users_large.*)
  FROM (
    SELECT *
    FROM staging_chat_users_large
    WHERE is_processed = false
    ORDER BY created_at ASC
    LIMIT 1000
  ) chat_users_large
  INTO users;

  -- Insert users
  IF ARRAY_LENGTH(users, 1) > 0 THEN
    INSERT INTO chat_users_large
    SELECT * FROM UNNEST(users);

    -- Mark inserted rows as processed
    UPDATE staging_chat_users_large
    SET is_processed = true
    WHERE id IN (SELECT id FROM UNNEST(users) u WHERE u.id IS NOT NULL);
  END IF;
END;
$BODY$
  LANGUAGE plpgsql VOLATILE;

--run every 5 seconds
CREATE EXTENSION pg_cron;
SELECT cron.schedule('*/5 * * * * *', 'SELECT insert_users()');

--check
SELECT * FROM cron.job_run_details;

-- stop cron job, type job name as parameter.
SELECT cron.unschedule(1);

--clean table
DELETE FROM cron.job_run_details;


SELECT cron.schedule('test', '*/5 * * * * *', 'SELECT insert_users()');

-- you can just view in table editor under schema cron.



CREATE OR REPLACE FUNCTION insert_users(users chat_users_large[])
  RETURNS void AS
$BODY$
DECLARE
BEGIN
  -- Insert users
  IF ARRAY_LENGTH(users, 1) > 0 THEN
    INSERT INTO chat_users_large
    SELECT * FROM UNNEST(users);

    -- Mark inserted rows as processed
    UPDATE staging_chat_users_large
    SET is_processed = true
    WHERE id IN (SELECT id FROM UNNEST(users) u WHERE u.id IS NOT NULL);
  END IF;
END;
$BODY$
  LANGUAGE plpgsql VOLATILE;

SELECT cron.schedule('insert_users_job', '*/5 * * * * *', 'SELECT * FROM insert_users(ARRAY[]::chat_users_large[])');
-------

CREATE TABLE chat_users_large (
uid uuid PRIMARY KEY DEFAULT uuid_generate_v4(),
name TEXT,
email TEXT,
signature TEXT,
avatar_url TEXT,
date_created TIMESTAMPTZ,
contacts TEXT[] DEFAULT '{}'
);
CREATE INDEX name_idx_cul ON chat_users_large (name);
CREATE INDEX email_idx_cul ON chat_users_large (email);

CREATE OR REPLACE FUNCTION insert_users_batch(users chat_users_large[])
  RETURNS void AS
$BODY$
BEGIN
  -- Insert users
  IF ARRAY_LENGTH(users, 1) > 0 THEN
    INSERT INTO chat_users_large
    SELECT * FROM UNNEST(users);
  END IF;
END;
$BODY$
  LANGUAGE plpgsql VOLATILE;

SELECT cron.schedule('insert_users_batch_job', '*/5 * * * * *', 'SELECT * FROM insert_users_batch(ARRAY[]::chat_users_large[])');

DROP TABLE IF EXISTS chat_users_large cascade;
DROP INDEX IF EXISTS name_idx_cul;
DROP INDEX IF EXISTS email_idx_cul;
select cron.unschedule('insert_users_batch_job');
DROP FUNCTION IF EXISTS insert_users_batch(users chat_users_large[]);

CREATE OR REPLACE FUNCTION insert_users_batch(users chat_users_large[])
  RETURNS void AS
$BODY$
DECLARE
  uid uuid;
BEGIN
  -- Insert users
  IF ARRAY_LENGTH(users, 1) > 0 THEN
    SELECT uuid_generate_v4() INTO uid;
    INSERT INTO chat_users_large (uid, name, email, signature, avatar_url, date_created, contacts)
    SELECT uid, name, email, signature, avatar_url, date_created, contacts
    FROM UNNEST(users);
  END IF;
END;
$BODY$
  LANGUAGE plpgsql VOLATILE;

CREATE EXTENSION IF NOT EXISTS "uuid-ossp";